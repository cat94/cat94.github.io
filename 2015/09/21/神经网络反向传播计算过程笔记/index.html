<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Ben Wong,wenbinben@gmail.com"><title>神经网络反向传播计算过程笔记 · 沈小黑的菜园</title><meta name="description" content="神经网络示意


反向传播计算对于每一个训练样本，进行如下循环:

正向传播计算出$a^{(2)}、z^{(2)}、a^{(3)}、z^{(3)}$等结果。正向传播计算的方法很简单，在此不再赘述。
计算$\delta_k^{(3)}=(a_k^{(3)}-y_k)$，这表明最后的计算结果和真实结果之"><meta name="keywords" content="Hexo,HTML,Ben,CSS,安卓,android,Linux,linuxdeepin"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">沈小黑的菜园</a></h3><div class="description"><p>Nothing lasts forever.</p></div></div></div><ul class="social-links"><li><a href="https://twitter.com/Ben_wenbin"><i class="fa fa-twitter"></i></a></li><li><a href="http://instagram.com/hwbinbenben"><i class="fa fa-instagram"></i></a></li><li><a href="/atom.xml"><i class="fa fa-rss"></i></a></li><li><a href="http://weibo.com/ben0036"><i class="fa fa-weibo"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai</a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"> </a></li></div><div class="avatar"><img src="https://secure.gravatar.com/avatar/e71df8021446fe9759a9928b1dd5c28d?s=180&amp;r=G&amp;d="></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>神经网络反向传播计算过程笔记</a></h3></div><div class="post-content"><h1 id="神经网络示意"><a href="#神经网络示意" class="headerlink" title="神经网络示意"></a>神经网络示意</h1><img title="神经网络示例图1" alt="神经网络示例图1" src="http://7xltls.com1.z0.glb.clouddn.com/images/machine_learning/神经网络/nueral_network_example.jpg">
<img title="神经网络示例图2" alt="神经网络示例图2" src="http://7xltls.com1.z0.glb.clouddn.com/images/machine_learning/神经网络/nueral_network_example2.jpg">
<a id="more"></a>
<h1 id="反向传播计算"><a href="#反向传播计算" class="headerlink" title="反向传播计算"></a>反向传播计算</h1><p>对于每一个训练样本，进行如下循环:</p>
<ol>
<li>正向传播计算出$a^{(2)}、z^{(2)}、a^{(3)}、z^{(3)}$等结果。正向传播计算的方法很简单，在此不再赘述。</li>
<li>计算$\delta_k^{(3)}=(a_k^{(3)}-y_k)$，这表明最后的计算结果和真实结果之间的误差。</li>
<li>根据$\delta_k^{(3)}$计算$\delta_k^{(2)}$，这里的计算公式为：$\delta_k^{(2)} = (\theta^{(2)})^T\delta^{(3)}.*g’(z^{(2)})$</li>
<li>记$\Delta^{(l)}$是一个$m*n$的矩阵，将$\delta^{(l+1)}(a^{(l)})^T$加到$\Delta^{(l)}$中。在这里，当前层共有n个元素，下一层共m个元素(不包括bias元素)。这里$\delta^{(l+1)}(a^{(l)})^T$中的第(i,j)个元素就表示在这个训练样本中第j个元素对下一层第i个元素误差的责任。</li>
</ol>
<p>最后，$\frac{\Delta{i,j}^{(l)}}{m}$等于$\frac{\partial}{\partial{\theta_{ij}^{(l)}}}J(\theta)$,即是$J(\theta)$关于$\theta_{ij}^{(l)}$的偏导数。</p>
<h1 id="正则化参数"><a href="#正则化参数" class="headerlink" title="正则化参数"></a>正则化参数</h1><p>上一部分，完成了最复杂的反向传播过程，到这个部分，只需要加上正则化参数$\frac{\lambda}{m}\sum_{j=1}^{end}{(\theta_{(ij)}^2)}$即可(依然要注意，这里bias单元的$\theta$不参与正则化)</p>
<p>$$\frac{\partial}{\partial{\theta_{ij}^{(l)}}}J(\theta)$ = $\frac{\Delta_{ij}^{(l)}}{m},j=0$$<br>$$\frac{\partial}{\partial{\theta_{ij}^{(l)}}}J(\theta)$ = $\frac{\Delta_{ij}^{(l)}}{m}+\frac{\lambda}{m}\sum_{j=1}^{end}{(\theta_{(ij)}^2)},j\geq1$$</p>
<p>到此，就完成了$J(\theta)$的偏导的计算了，有了这个结果之后，就可以使用梯度下降的思想来优化$\theta$了。</p>
<h1 id="其它注意"><a href="#其它注意" class="headerlink" title="其它注意"></a>其它注意</h1><p>神经网络还有很多需要注意的地方，试举出几例：</p>
<ol>
<li>参数$\theta$的初始化不能跟线性回归的时候一样随便取0即可，而要进行随机化处理，因为如果所有的$\theta$值都相同，那么到时候进行计算的时候，所有的单元影响值都一样，进行的其实都是相同的计算，没有任何差别。</li>
<li>进行反向传播计算的时候，偏置单元是不需要计算在误差影响中的，因为偏置单元1作为一个常量，显然只是提供一个数值起点，对它进行误差调整没有意义。</li>
<li>反向传播算法由于比较复杂，所以结果的检验就显得特别重要。在《机器学习》课程中使用的验算方法是在目标点附近取极小的区间[$x-\epsilon,x+\epsilon$],计算近似导数$\frac{J(x+\epsilon)-J(x-\epsilon)}{2\epsilon}$($\epsilon$通常取很小的值，如$10^{-9}$)，比较使用反向传播法计算的偏导是否和这个近似值相近。</li>
</ol>
<p>神经网络是看《机器学习》在线课程到现在为止的一个难点，特此记录。</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2015-09-21</span><i class="fa fa-tag"></i><a href="/tags/机器学习/" title="机器学习" class="tag">机器学习 </a><a href="/tags/课程学习/" title="课程学习" class="tag">课程学习 </a></div></div></div></div><div class="share"><div class="evernote"> <a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"> <a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"> <a href="http://twitter.com/home?status=,http://www.shenjianan.net/2015/09/21/神经网络反向传播计算过程笔记/,沈小黑的菜园,神经网络反向传播计算过程笔记,;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a role="navigation" href="/2015/09/21/hexo的坑/" title="hexo的坑" class="btn">上一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>